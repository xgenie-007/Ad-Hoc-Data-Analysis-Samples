{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBJECTIVE\n",
    "You are trying to determine the 7-year survival of prostate cancer patients. A patient survived if they are still alive 7 years after diagnosis. This means that a patient is counted as dead whether or not the death was due to their cancer. You have been given details about the patients and their cancers to help you with your prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection: Random Forest Classifier in Scikit-Learn\n",
    "Machine Learning techniques can be used for this modeling problem. Bearing in mind that this is ultimately a classification problem, a Random Forest Classifier was deemed to be the most favourable approach because:\n",
    "\n",
    "1. A Logistic Regression model would assume some linearity within the data. However, a quick exploratory analysis showed that there is not much linear correlation between the columns, and the target ('survival_7_years'). On the other hand, Ensemble methods like Random Forest do not. Also, Logistic Regression can hardly handle categorical (binary) features, and we expect to have some in this dataset.\n",
    "2. Support Vector Machine is inefficient to train, and would need normalising across the data set. This could be fun to explore, but, given the time constraint, a random forest classifier would do the job without requiring those extra manipulations. \n",
    "3. Gradient Boosted Decision Trees could perform better, but they are prone to over-fitting, and need a lot of their hyper parameters to be tuned perfectly, to get right. Again, this is something interesting to explore, but for the sake of time, Random Forest could perform the job without the fuss.\n",
    "\n",
    "Howver, after modelling, it was demonstrated that Gradient Boosted Decision Trees outperformed Random Forests by 2 points, making them a more suitable choice for this project.\n",
    "\n",
    "### General Strategy\n",
    "1. Check the target column, to ensure that the data is balanced. If it is unbalanced, i.e, the ratio of positive to negative classes is skewed, then sampling measures have to be taken to improve the model's sensitivity.\n",
    "\n",
    "\n",
    "2. Clean the data: Because scikit-learn's ensenble methods do not deal will with missing values (NaN), values will have to be imputed where missing. Note: It will be interesting to compare the performance of a model with imputed data, to that of an R model, which handles missing values. \n",
    "\n",
    "3. Make dummy columns for categorical variables (such as the symptoms column), so the Random Forest can handle them. \n",
    "\n",
    "4. Train and retrain the model, using k-fold cross validation, as well as feature selection methods, to minimise bias and variance.\n",
    "\n",
    "5. Iterate, based on performance. If satisfactory, then fit the model on the test data provided.\n",
    "\n",
    "6. Take a deep breath, stop overthinking scenarios, and step way from the problem :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis_date</th>\n",
       "      <th>gleason_score</th>\n",
       "      <th>t_score</th>\n",
       "      <th>n_score</th>\n",
       "      <th>m_score</th>\n",
       "      <th>stage</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>height</th>\n",
       "      <th>...</th>\n",
       "      <th>symptoms</th>\n",
       "      <th>rd_thrpy</th>\n",
       "      <th>h_thrpy</th>\n",
       "      <th>chm_thrpy</th>\n",
       "      <th>cry_thrpy</th>\n",
       "      <th>brch_thrpy</th>\n",
       "      <th>rad_rem</th>\n",
       "      <th>multi_thrpy</th>\n",
       "      <th>survival_1_year</th>\n",
       "      <th>survival_7_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jun-05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>T1c</td>\n",
       "      <td>N0</td>\n",
       "      <td>M0</td>\n",
       "      <td>I</td>\n",
       "      <td>86.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>U03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Feb-06</td>\n",
       "      <td>8.0</td>\n",
       "      <td>T3a</td>\n",
       "      <td>N1</td>\n",
       "      <td>M0</td>\n",
       "      <td>IV</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>U06,S07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mar-06</td>\n",
       "      <td>9.0</td>\n",
       "      <td>T1a</td>\n",
       "      <td>N0</td>\n",
       "      <td>M0</td>\n",
       "      <td>IIB</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>U01,U02,U03,S10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Feb-05</td>\n",
       "      <td>8.0</td>\n",
       "      <td>T2b</td>\n",
       "      <td>N0</td>\n",
       "      <td>M0</td>\n",
       "      <td>IIB</td>\n",
       "      <td>86.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>U01,U02,S10,O11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dec-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>T4</td>\n",
       "      <td>N0</td>\n",
       "      <td>M0</td>\n",
       "      <td>IV</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>U01,U03,U05,S07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id diagnosis_date  gleason_score t_score n_score m_score stage   age  race  \\\n",
       "0   1         Jun-05            4.0     T1c      N0      M0     I  86.0   4.0   \n",
       "1   2         Feb-06            8.0     T3a      N1      M0    IV  66.0   2.0   \n",
       "2   3         Mar-06            9.0     T1a      N0      M0   IIB  84.0   4.0   \n",
       "3   4         Feb-05            8.0     T2b      N0      M0   IIB  86.0   3.0   \n",
       "4   5         Dec-01            8.0      T4      N0      M0    IV  78.0   4.0   \n",
       "\n",
       "   height        ...                symptoms  rd_thrpy  h_thrpy  chm_thrpy  \\\n",
       "0    66.0        ...                     U03         0        0          1   \n",
       "1    70.0        ...                 U06,S07         1        1          1   \n",
       "2    69.0        ...         U01,U02,U03,S10         1        1          0   \n",
       "3    69.0        ...         U01,U02,S10,O11         0        0          0   \n",
       "4    70.0        ...         U01,U03,U05,S07         1        1          1   \n",
       "\n",
       "   cry_thrpy brch_thrpy  rad_rem  multi_thrpy  survival_1_year  \\\n",
       "0          1          0        1            1                1   \n",
       "1          0          0        0            1                1   \n",
       "2          0          1        1            1                1   \n",
       "3          1          0        1            1                0   \n",
       "4          0          0        0            1                1   \n",
       "\n",
       "   survival_7_years  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "raw_train = pd.read_csv('training_data.csv')\n",
    "test_df = pd.read_csv('(name)_score.csv')\n",
    "output_df = pd.read_csv('(name)_score.csv')\n",
    "raw_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43230419239519013"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.loc[raw_train['survival_7_years'] == 1].shape[0]/raw_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With over 43% in the positive class, this dataset is not imbalanced, and can be trained without sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "Eyeballing at the dataset, there are missing values at various columns. One method would be just to drop all rows containing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15924601884952877"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_check = raw_train.dropna()\n",
    "a_check.shape[0]/raw_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, from the simple calculation above, it shows that dropping all missing values from the dataframe will leave us with only 16% of our data, for training a model. Now, there are some issues with this:\n",
    "1. The data that we will be testing our model on might not be perfect, and could have a lot of missing values. The model is expected to make predictions for the entire data set, not just cherry-picked, perfect rows.\n",
    "2. Even if the data we are testing is perfect, the trained model used only 16% of the data available to us, and will not be as accurate as a case where a model is built based on a larger data set. \n",
    "\n",
    "Bearing this in mind, it is prudent to clean the entire dataset before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>missing values train %</th>\n",
       "      <th>missing values test %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diagnosis_date</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gleason_score</td>\n",
       "      <td>2.079948</td>\n",
       "      <td>2.072674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_score</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_score</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m_score</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stage</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>4.861878</td>\n",
       "      <td>5.619634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>race</td>\n",
       "      <td>1.072473</td>\n",
       "      <td>1.049345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>height</td>\n",
       "      <td>8.865778</td>\n",
       "      <td>9.045183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>weight</td>\n",
       "      <td>8.560286</td>\n",
       "      <td>9.027838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>family_history</td>\n",
       "      <td>10.308742</td>\n",
       "      <td>10.155234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>first_degree_history</td>\n",
       "      <td>10.308742</td>\n",
       "      <td>10.155234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>previous_cancer</td>\n",
       "      <td>10.308742</td>\n",
       "      <td>10.155234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smoker</td>\n",
       "      <td>10.308742</td>\n",
       "      <td>10.155234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>side</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tumor_diagnosis</td>\n",
       "      <td>1.969451</td>\n",
       "      <td>1.821178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tumor_6_months</td>\n",
       "      <td>65.407865</td>\n",
       "      <td>65.519036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tumor_1_year</td>\n",
       "      <td>13.799155</td>\n",
       "      <td>13.910329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>psa_diagnosis</td>\n",
       "      <td>9.086773</td>\n",
       "      <td>8.923771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>psa_6_months</td>\n",
       "      <td>61.767956</td>\n",
       "      <td>61.972075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>psa_1_year</td>\n",
       "      <td>16.360091</td>\n",
       "      <td>16.607406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tea</td>\n",
       "      <td>10.308742</td>\n",
       "      <td>10.155234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>symptoms</td>\n",
       "      <td>2.664933</td>\n",
       "      <td>2.809817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rd_thrpy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>h_thrpy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>chm_thrpy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cry_thrpy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>brch_thrpy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rad_rem</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>multi_thrpy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>survival_1_year</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.544706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>survival_7_years</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 columns  missing values train %  missing values test %\n",
       "0                     id                0.000000               0.000000\n",
       "1         diagnosis_date                0.000000               0.000000\n",
       "2          gleason_score                2.079948               2.072674\n",
       "3                t_score                0.000000               0.000000\n",
       "4                n_score                0.000000               0.000000\n",
       "5                m_score                0.000000               0.000000\n",
       "6                  stage                0.000000               0.000000\n",
       "7                    age                4.861878               5.619634\n",
       "8                   race                1.072473               1.049345\n",
       "9                 height                8.865778               9.045183\n",
       "10                weight                8.560286               9.027838\n",
       "11        family_history               10.308742              10.155234\n",
       "12  first_degree_history               10.308742              10.155234\n",
       "13       previous_cancer               10.308742              10.155234\n",
       "14                smoker               10.308742              10.155234\n",
       "15                  side                0.000000               0.000000\n",
       "16       tumor_diagnosis                1.969451               1.821178\n",
       "17        tumor_6_months               65.407865              65.519036\n",
       "18          tumor_1_year               13.799155              13.910329\n",
       "19         psa_diagnosis                9.086773               8.923771\n",
       "20          psa_6_months               61.767956              61.972075\n",
       "21            psa_1_year               16.360091              16.607406\n",
       "22                   tea               10.308742              10.155234\n",
       "23              symptoms                2.664933               2.809817\n",
       "24              rd_thrpy                0.000000               0.000000\n",
       "25               h_thrpy                0.000000               0.000000\n",
       "26             chm_thrpy                0.000000               0.000000\n",
       "27             cry_thrpy                0.000000               0.000000\n",
       "28            brch_thrpy                0.000000               0.000000\n",
       "29               rad_rem                0.000000               0.000000\n",
       "30           multi_thrpy                0.000000               0.000000\n",
       "31       survival_1_year                0.000000              49.544706\n",
       "32      survival_7_years                0.000000             100.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(raw_train)\n",
    "explore_table = pd.DataFrame()\n",
    "explore_table['columns'] = pd.Series(columns)\n",
    "for i in range(len(columns)):\n",
    "    explore_table.ix[i,'missing values train %'] = (raw_train.loc[raw_train[columns[i]].isnull()].shape[0]\n",
    "                                                    /raw_train.shape[0])*100\n",
    "    explore_table.ix[i,'missing values test %'] = (test_df.loc[test_df[columns[i]].isnull()].shape[0]\n",
    "                                                 /test_df.shape[0])*100\n",
    "explore_table.head(len(columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that both the test and training data have missing values in the same colums, except for 'survival_1_year', and 'survival_7-years', where the test set has a lot of missing values. \n",
    "#### Take-aways:\n",
    "1. There is clearly some correlation between a patient surviving the first year, and a patient surviving 7 years, as a patient dead after 1 year is clearly dead after 7. However, with 50% of this data missing in the test set, we will either (a) have to ignore this as a feature in our model, or (b) predict 1st year survival, and then use it as a feature. to err on the side of caution, if our model performs fairly well, this column should be ignored.  \n",
    "\n",
    "2. While the missing values for weight and height are close to 10% in both cases, there is a correlation between weight and height. We can use this to impute values for weight and height, and keep them as a feature. However, the BMI relationship between weight and height is likely what drives relevance to a patient's survival. As such, imputation might mess with this data.\n",
    "\n",
    "3. tumor 6 months, psa 6 months, tumor 1 year, psa 1 year, and psa diagnosis should be left out as they have a significant number of missing values.\n",
    "\n",
    "4. While family history, tea, smoker, previous cancer and first degree history have ~10% missing values, their %age of missing values are the same! The patients who have missing values in each of these fields, have missing values in ALL of them. This implies that they are not missing at random, and categorical values can be assigned where the values are NaN in these columns.\n",
    "\n",
    "5. For all other missing value columns, imputation techniques can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "missing_type_a = ['gleason_score','age','tumor_diagnosis']\n",
    "missing_type_b = ['tea','race','family_history', 'first_degree_history', 'previous_cancer', 'smoker']\n",
    "\n",
    "\n",
    "### Use medians to fill in missing type a\n",
    "for i in missing_type_a:\n",
    "    temp = raw_train[i].dropna()\n",
    "    med = temp.median()\n",
    "    raw_train[i].fillna(med, inplace=True)\n",
    "\n",
    "for i in missing_type_b:\n",
    "    raw_train[i].fillna(-999,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Training Set & Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = pd.DataFrame()\n",
    "training_set['id']=raw_train['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineer the Categorical Columns, and add non-categorical features:\n",
    "\n",
    "A lot of the sections in the dataset are categorical, especially the symptoms column, where it is comma separated. A method to deal with such categorical data for ensemble methods, is the use of dummy columns for each category, with a '1' indicating positive for that category, and '0', otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "categorical_columns = ['t_score', 'n_score', 'm_score', 'stage','race','family_history', 'first_degree_history', \n",
    "                       'previous_cancer', 'smoker', 'side','rd_thrpy', 'h_thrpy', \n",
    "                       'chm_thrpy','cry_thrpy', 'brch_thrpy', 'rad_rem', 'multi_thrpy']\n",
    "other_columns = ['gleason_score','age','tumor_diagnosis','tea']\n",
    "for i in categorical_columns:\n",
    "    training_set = pd.concat([training_set, pd.get_dummies(raw_train[i], prefix=i)], axis = 1)\n",
    "for j in other_columns:\n",
    "    training_set = pd.concat([training_set, raw_train[j]], axis = 1)\n",
    "training_set = pd.concat([training_set, raw_train['symptoms'].str.get_dummies(sep=',')], axis=1)\n",
    "training_set = pd.concat([training_set, raw_train['survival_7_years']], axis = 1)\n",
    "training_set.set_index('id')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(training_set.columns)\n",
    "features.remove('id')\n",
    "features.remove('survival_7_years')\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 83 features with which to train our random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Turing1/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62897985705003245, 0.59974009096816117, 0.63872644574398962, 0.64262508122157247, 0.61663417803768683, 0.61313394018205458, 0.61833550065019505, 0.63068920676202855, 0.63459037711313393, 0.6248374512353706]\n",
      "0.624829212896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(len(training_set), n_folds=10, shuffle=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for traincv, testcv in kf:\n",
    "    train = training_set.iloc[traincv] # Extract train data with cv indices\n",
    "    valid = training_set.iloc[testcv] # Extract valid data with cv indices\n",
    "    model = clf.fit(X = train[features], y = train['survival_7_years'])\n",
    "    score = clf.score(X = valid[features], y = valid['survival_7_years'])\n",
    "    results.append(score)\n",
    "\n",
    "print(results)\n",
    "print(sum(results)/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64912280701754388, 0.63807667316439243, 0.65497076023391809, 0.66016894087069522, 0.63807667316439243, 0.64434330299089726, 0.64759427828348504, 0.62873862158647598, 0.64954486345903772, 0.64304291287386217]\n",
      "0.645367983364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(len(training_set), n_folds=10, shuffle=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for traincv, testcv in kf:\n",
    "    train = training_set.iloc[traincv] # Extract train data with cv indices\n",
    "    valid = training_set.iloc[testcv] # Extract valid data with cv indices\n",
    "    model = clf.fit(X = train[features], y = train['survival_7_years'])\n",
    "    score = clf.score(X = valid[features], y = valid['survival_7_years'])\n",
    "    results.append(score)\n",
    "\n",
    "print(results)\n",
    "print(sum(results)/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not perform as well as we would like, but comparing its score to a logistic model, SVM, and neural network, as well as extra trees classifier and other ensemble methods, the random forest classifier performs better than them. However, Gradient Boosting performs slightly better by 2 points.\n",
    "\n",
    "### Making predictions\n",
    "Using the same process we did for the training set, we can engineer features for the test set (scored set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imputation\n",
    "\n",
    "### Use medians to fill in missing type a\n",
    "for i in missing_type_a:\n",
    "    temp = test_df[i].dropna()\n",
    "    med = temp.median()\n",
    "    test_df[i].fillna(med, inplace=True)\n",
    "\n",
    "for i in missing_type_b:\n",
    "    test_df[i].fillna(-999,inplace=True)\n",
    "    \n",
    "### Feature Engineering\n",
    "\n",
    "test_set = pd.DataFrame()\n",
    "test_set['id']=test_df['id']\n",
    "\n",
    "for i in categorical_columns:\n",
    "    test_set = pd.concat([test_set, pd.get_dummies(test_df[i], prefix=i)], axis = 1)\n",
    "for j in other_columns:\n",
    "    test_set = pd.concat([test_set, test_df[j]], axis = 1)\n",
    "test_set = pd.concat([test_set, test_df['symptoms'].str.get_dummies(sep=',')], axis=1)\n",
    "test_set = pd.concat([test_set, test_df['survival_7_years']], axis = 1)\n",
    "\n",
    "test_set.set_index('id')  \n",
    "\n",
    "### get feature columns, and confirm that they match those of the training set\n",
    "features_test = list(training_set.columns)\n",
    "features_test.remove('id')\n",
    "features_test.remove('survival_7_years')\n",
    "len(features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train = training_set[features]\n",
    "X_test = test_set[features]\n",
    "y_train = training_set['survival_7_years']\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "test_df['survival_7_years'] = model.predict(X_test)\n",
    "output_df['survival_7_years'] = test_df['survival_7_years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_df.to_csv('KaykayEssien_score.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
